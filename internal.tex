\section{Motivation}\label{s:motivation}

% Fragmentation becomes more pronounced in high performance systems

% 1. Interfacing overhead is getting a larger portion
% 2. Complicated architectures make unexpected additional fragmentations

% However, studying fragmentation is difficult

% 1. Time a lot of efforts:
%    - Take weeks/months/years to age as you wants
%    - Running an experiment will distort the condition, setting again take long
%
% 2. Reproductability
%    - No standard procedure
%    - Different run-to-run results make it difficult to reproduce evaluation
%
% 3. Both logical and physical fragmentation matters
%    - Due to the hw-built nature of storage...
%


% MOVED --------------------------------------------------vvvvvvvvvvvvvvvv
Logical and physical fragmentation can occur independently, and appropriate measures are needed to address each issue.

% Fragmentation in device types
% HDD: logical
In the era of HDDs, the main cause of performance degradation due to file fragmentation is seek time and rotational latency.
In HDDs, there is a need for seek time to move the disk head to the track where the requested sector is located, as well as rotational latency to find it on that track.
Fragmentation increases the overhead by requiring the disk head to move more frequently, particularly having a pronounced negative impact on read operations.
Write performance can be optimized with prefetching and bufferred operations.

% SSD: logical + physical
In flash memory, there is no seek time or rotational latency, so early researchers generally argued that SSD performance is not affected by filesystem fragmentation.
Therefore, techniques like defragmentation were thought to have only negative impacts by reducing the lifespan of flash memory due to additional write operations~\cite{defrag-mobile:atc17,fragpicker:sosp21,defrag-lfs:apsys16,no-afraid:fast24,parallel-defrag:sac22,Defragmentation_read_collision}.
However, more recent studies have shown that filesystem aging and resultant fragmentation can also lead to performance degradation in SSDs~\cite{Problem_in_SSD_Empirical,senescence:fast17,Problem_in_SSD_Mobile_Devices,survey:ictc23}.
For example, in the storage devices built with NAND flash memory, data can be distributed across multiple channels, which directly impacts I/O parallelism.
built with high-density high-performance media with complicated architecture.

The first reason for this is request fragmentation due to logical fragmentation.
In the Linux I/O stack, a single I/O request can only represent contiguous LBA.
Fragmentation influences the non-contiguous storage of data, causing a single I/O request to be divided into multiple smaller I/O requests.\cite{IO}
This increases the total number of I/O requests, and all requests derived from a single I/O must be completed before the next process can proceed, leading to issues in I/O performance.
If multiple I/O requests are necessary for one file, the system must generate bio structures, request structures, and I/O commands corresponding to the number of requests.
This process incurs overhead in I/O request management.

Secondly, SSDs enhance read performance through prefetching, which loads subsequent data in advance based on LBA.
Fragmentation can cause this prefetching to load unnecessary data.\cite{defrag-lfs:apsys16} For these reasons, filesystem fragmentation is also a concern in SSDs.
% MOVED --------------------------------------------------^^^^^^^^^^^^^^^^


Despite the emergence of various new types of storage devices developed in response to the increasing demand for high-performance and efficient storage solutions, the issue of fragmentation continues to persist in storage devices.
This problem of fragmentation can lead to degraded performance and aging of the storage devices.

When new research is conducted, new SSDs are expected to have the best performance with no fragmentation whatsoever.
However, SSDs used in real life often experience performance degradation due to prolonged use and significant fragmentation.
To address this, SSDs are aged to induce the desired level of fragmentation, allowing for a more accurate representation of the conditions in which SSDs are used.

In the case of HDDs, aging is relatively straightforward because the physical area and logical area coincide.
In contrast, SSDs cannot be accessed externally due to security reasons related to the Flash Translation Layer (FTL), necessitating indirect aging.
This means that researchers cannot directly modify data placement and must perform aging by reading and writing meaningless data to the SSD for several days.
This process is time-consuming and demands a lot of efforts to incorporate the fragmentation into research.

% What we need: fast direct approach
This motivation led to the idea of reducing the time required for aging by directly aging the SSD, retrieving data from the FTL, and storing it in files.
These files can then be loaded into the FTL of a new SSD to quickly configure an aged SSD.
To achieve this, we attempted to store the FTL information and storage space data of a virtual SSD on a storage emulator called NVMeVirt, allowing for loading into a new SSD.
We argue that this approach will significantly reduce the time spent on aging tasks conducted to align with the actual user environment before research, making it an essential tool for researchers.



%-------------------------------------------------------------------------
\section{Internal}\label{s:internal}

%-------------------------------------------------------------------------
\subsection{NVMeVirt}

\begin{comment}
NVMeVirt\cite{nvmevirt} is a Linux kernel module that provides a virtual NVMe device to the system, supporting various types of SSDs including basic SSDs, ZNS SSDs, and key-value SSDs.
The device is emulated at the PCI layer and can be used not only for standard devices but also for advanced storage configurations such as NVMe-over-Fabrics offloads.

The storage space of modern SSDs is divided into multiple partitions, as shown in Figure \ref{fig:structure}, with each partition managed by a separate FTL instance.
As a result, multiple FTL instances exist within a single SSD, sharing one PCIe link connected to the host.
Each partition consists of several NAND channels, and each channel is connected to multiple dies.
This architecture allows for data transfer through NAND channels to be serialized, while each die can operate independently.
In this structure, the FTL coordinates the dies and channels to handle I/O requests.
Thanks to this design, a single I/O request can be split into smaller tasks and distributed across multiple FTL instances, allowing for parallel processing.

As the data density per cell increases, the NAND programming time has also increased.
To hide this time, a write buffer is employed.
The data from incoming write requests is stored in the write buffer, and the write request is considered complete when the payload is copied to the write buffer.
If this write buffer becomes full, the data must be processed.
However, in NVMeVirt, to facilitate one-shot programming, this data is not immediately processed.
Instead, it waits until the buffering of another logical page in the same physical page is completed, after which the data is moved to the corresponding FTL for storage.
Once the write operation is complete, the FTL instance is reclaimed.
This processing occurs in the background.
For reads, the data is sent directly from the FTL instance to the buffer specified in the NVMe command without passing through the write buffer.

The storage of NVMeVirt is located in memory.
Since a significant amount of memory is required for data storage, it is necessary to reserve a portion of the physical address space at system initialization through boot parameters, as shown in Figure \ref{fig:making}.
Additionally, a simple linear mapping is used for data placement based on a conventional SSD.
Therefore, the memory location for the physical block and page numbers in the flash address space is calculated by adding the starting address of the reserved memory area.

There are several tools available for simulating or emulating SSDs.
Simulators use data processing models to mimic the internal operations of actual devices.
Their primary purpose is to construct models of the devices and parameterize the performance of internal operations to calculate desired performance metrics in the model.
As a result, while detailed analysis is possible, executing actual workloads can slow down the overall system when simulating the entire system.

Many emulators similar to NVMeVirt exist; for instance, FlexDrive can only emulate existing SSD types and does not support devices like KVSSDs and ZNS SSDs.
Our research aimed to reduce the time required for aging when conducting new studies, so we sought an emulator that could be used across various devices and thus did not utilize those emulators.
FEMU, on the other hand, provides a virtual NVMe device to guest operating systems by leveraging device virtualization features.
However, in a virtual machine environment, the guest physical memory is scattered across the host's physical memory through the host's virtual memory system, which can be inconvenient, and there is also the hassle of needing to reboot during experiments.

In contrast, NVMeVirt allows developers to easily modify the NVMe interface layer, enabling exploration of various design spaces and support for new device types.
Additionally, because it does not rely on virtualization technology, it permits comprehensive communication models with low overhead, and its modular design allows for easy usage through module loading.
For these reasons, we conducted our research on NVMeVirt.

Using an emulator like NVMeVirt enables direct access to the FTL, making our research feasible.
Through our research, we aim to facilitate the rapid creation of aged SSDs for future studies.
\end{comment}

% intro => What is NVMeVirt
% NVMeVirt는 소프트웨어로 정의된 NVMe 장치로, 리눅스 커널 모듈로 구현되어 있으며, 기존 NVMe 인터페이스를 사용하여 다양한 저장 장치를 모방하는 기능을 제공합니다.
% 주 목적은 고급 저장 장치를 개발하고 연구할 수 있는 환경을 제공하는 것으로, 실제 저장 장치와 유사한 방식으로 동작하게 됩니다.

NVMeVirt is one of the modern storage device emulators.
As a Linux kernel module, NVMeVirt provides the capability to emulate various storage devices using the existing NVMe interface, realizing software-defined NVMe devices.
The primary objective is to offer an environment for developing and researching advanced storage devices, operating in a manner similar to actual storage devices.

% development => How Work?
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{ssd}
    \caption{NVMeVirt SSD structure}
    \label{fig:structure}
\end{figure}

% NVMeVirt는 다양한 구성 요소로 이루어진 소프트웨어 정의 NVMe 장치로, 내부 구조는 FTL(Flash Translation Layer) 인스턴스, 채널, 다이(die), 블록, 페이지를 포함하여 데이터 처리를 수행합니다.
% PCIe 링크를 통해 직렬화된 데이터 전송에도 불구하고, FTL은 여러 NAND 채널과 다이를 통해 데이터 I/O 요청을 병렬로 처리합니다.
% 이러한 아키텍처에서 FTL은 다이와 채널을 조정하여 효율적인 데이터 처리를 진행하며, 각 다이는 독립적으로 작동하여 성능을 극대화합니다.

NVMeVirt is a software-defined NVMe device composed of various components, with its internal structure performing data processing through instances of the Flash Translation Layer (FTL), channels, dies, blocks, and pages.
Despite serialized data transmission over the PCIe link, the FTL processes data I/O requests in parallel through multiple NAND channels and dies.
In this architecture, the FTL coordinates the dies and channels to enable efficient data processing, with each die operating independently to maximize performance.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{nvmev-making}
    \caption{Make the NVMeVirt instance}
    \label{fig:making}
\end{figure}

% 시스템 초기화 시 물리적 주소 공간의 일부를 예약하여 필요한 메모리를 확보하고, 이 예약된 메모리는 I/O 작업자, NVMeV 코어 디스패처 및 SSD 코어 등 NVMeVirt의 주요 구성 요소에 데이터를 저장하고 관리하는 데 사용됩니다.
% 사용자는 구성 파일을 통해 SSD의 종류를 지정하고 메모리의 시작과 끝 주소를 설정할 수 있어, 다채로운 구성 옵션을 선택택할 수 있습니다.

% 내부적으로 디스패처와 I/O 작업자가 상호작용하며 네이티브 NVMe 장치처럼 데이터를 처리합니다.
% 이러한 에뮬레이션 방식은 저장 장치 연구와 엔지니어링을 위한 다재다능함을 보여줍니다.
% 더불어 NVMeVirt는 다양한 저장 장치 유형인 일반 SSD, NVM SSD, ZNS SSD, 키-값 SSD를 지원하며, 다양한 저장 장치 인스턴스를 생성하여 연구자들이 여러 SSD 아키텍처를 실험하고 분석할 수 있는 기회를 제공합니다.

During system initialization, a portion of the physical address space is reserved to secure the necessary memory, which is utilized to store and manage data for the key components of NVMeVirt, such as the I/O worker, NVMeV core dispatcher, and SSD core.
Users can specify the type of SSD and set the starting and ending addresses of the memory through a configuration file, allowing for a variety of configuration options.

Internally, the dispatcher and I/O worker interact and process data like a native NVMe device.
This emulation approach demonstrates versatility for storage device research and engineering.
Additionally, NVMeVirt supports various types of storage devices, including standard SSDs, NVM SSDs, ZNS SSDs, and key-value SSDs, providing researchers with opportunities to create multiple SSD instances for experimentation and analysis of different SSD architectures.

% trun => Others

% SSD를 시뮬레이션하거나 에뮬레이션하기 위한 여러 도구는 이미 존재합니다.
% 시뮬레이터는 데이터 처리 모델을 사용하여 실제 장치의 내부 작업을 모방합니다.
% 이들의 주요 목적은 장치 모델을 구축하고 내부 작업의 성능을 매개변수화하여 모델에서 원하는 성능 메트릭을 계산하는 것입니다.
% 결과적으로, 상세한 분석이 가능하지만 전체 시스템을 시뮬레이션할 경우 실제 작업을 실행하는 데 시간이 지연될 수 있기 때문에 저희의 연구에서는 알맞지 않습니다.

% 에뮬레이터인 FlexDrive는 기존 SSD 유형만 에뮬레이션할 수 있고 KVSSD 및 ZNS SSD와 같은 장치는 지원하지 않습니다.
% 우리의 연구는 새로운 연구를 수행할 때 소요되는 노화 시간을 최소화하는 것을 목표로 하였기에, 다양한 장치에서 활용 가능한 에뮬레이터를 탐색하고자 하였습니다.
% 또 다른 에뮬레이터 장치 FEMU는 장치 가상화 기능을 활용하여 게스트 운영 체제에 가상 NVMe 장치를 제공합니다.
% 그러나 가상 머신 환경에서는 게스트 물리 메모리가 호스트의 가상 메모리 시스템을 통해 호스트의 물리 메모리에 분산되어 있어 불편할 수 있으며, 실험 중 재부팅이 필요하다는 번거로움이 있습니다.

Several tools already exist for simulating or emulating SSDs.
These simulators use data processing models to mimic the internal operations of actual devices.
Their main purpose is to build device models and parameterize the performance of internal operations to calculate the desired performance metrics from the models.
As a result, while detailed analysis is possible, simulating the entire system can introduce delays in executing actual tasks, making it unsuitable for our research.

The emulator FlexDrive can only emulate existing types of SSDs and does not support devices such as KVSSD and ZNS SSDs.
Since our research aimed to minimize the aging time required for conducting new studies, we sought to explore emulators that could be utilized across various devices.
Another emulator device, FEMU, leverages device virtualization to provide virtual NVMe devices to guest operating systems.
However, in a virtual machine environment, the guest physical memory is distributed across the host's physical memory via the host's virtual memory system, which can be inconvenient, and it also requires rebooting during experiments.

% conclusion => Advantages over others

% 반대로, NVMeVirt는 개발자가 NVMe 인터페이스 레이어를 손쉽게 수정할 수 있도록 하여 다양한 설계 공간을 탐색하고 새로운 장치 유형을 지원할 수 있는 유연성을 제공합니다.
% 또한, 가상화 기술에 의존하지 않기 때문에 낮은 오버헤드로 포괄적인 통신 모델을 구현할 수 있으며, 모듈식 설계를 통해 모듈 로딩이 용이합니다.
% 이러한 특성 덕분에 우리는 NVMeVirt를 기반으로 연구를 진행하게 되었습니다.
% 추가적으로, NVMeVirt를 활용함으로써 FTL에 직접 접근할 수 있는 부가적인 이점을 얻었습니다.

Conversely, NVMeVirt provides developers with the flexibility to easily modify the NVMe interface layer, allowing them to explore various design spaces and support new device types.
Additionally, by not relying on virtualization technology, it can implement a comprehensive communication model with low overhead, and the modular design facilitates easy module loading.
Thanks to these characteristics, we proceeded with our research based on NVMeVirt.
Furthermore, leveraging NVMeVirt offers the additional advantage of direct access to the FTL.

%-------------------------------------------------------------------------
\subsection{Implementation}

\subsubsection{What to Bring}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\textwidth]{mapping-table}
    \caption{NVMeVirt’s relation between FTL and data save process}
    \label{fig:mappingtable}
\end{figure*}

There are four main types of data that need to be retrieved from the aged SSD on NVMeVirt.
% 1. FTL
First, the metadata of the SSD related to the FTL must be obtained.
Unlike actual SSDs, the FTL in NVMeVirt primarily performs the task of calculating request times.
When storing data in actual storage, it uses Logical Block Addresses (LBA) instead of Physical Page Addresses (PPA), eliminating the need to convert LBA to PPA for disk I/O assistance.
Instead, as shown in Figure \ref{fig:mappingtable}, the LBA is abstractly related to the PPA, and the expected request time is calculated so that the time taken to store or read data matches the computed request time.
This allows for abstract handling of each PPA.
Therefore, it is necessary to store the mapping table, reverse mapping table, and various parameters used to calculate request times.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{ftl-role}
    \caption{FTL’s role}
    \label{fig:role}
\end{figure}

% 2. Block metadata
Second, we need to bring the Free Block list, Full Block list, and Victim Block queue that manage the Free and Full blocks.
The state of an SSD's blocks is divided into two categories: Full Blocks, which are completely written, and Free Blocks, which are empty.
The FTL manages these blocks in list form.
As shown in Figure \ref{fig:role}, when the LBA is converted to PPA within the FTL and the request time is calculated, the write pointer is placed in either the Victim Block queue or the Full Block List.
The Victim Block queue contains blocks that have data written but are not yet in the Full Block state.
Later, through Garbage Collection, the data in these blocks is cleared to transition them to Free Block status.
The FTL manages Free Blocks through the Free Block List, allocating blocks from this list whenever a new write pointer is needed.
In other words, the state of the SSD blocks is managed through the Free Block List, Full Block List, and Victim Block queue, which determines where to write data.
Therefore, it is essential to preserve this data regarding block states.

% 3. SSD Metadata
Third, metadata about the SSD itself, beyond the FTL, must be retrieved.
Since NVMeVirt is an emulator for SSDs, it manages metadata for channels, blocks, and pages.
Most of this data can be recreated in a new SSD without issues.
However, metadata for blocks and especially pages must be retained.
The page metadata includes the status of each page, which distinguishes whether the page is in a valid or invalid state.
When a new SSD is created, all pages are initialized to a valid state, so this information must be preserved.
For blocks, the metadata includes the count of valid pages, the count of invalid pages, and the erase count.
This information also needs to be maintained, as it will be reset in the new SSD.
Failing to retain the metadata for pages and blocks could lead to conflicts with the FTL's metadata, resulting in issues.

% 4. Contents
Lastly, the contents of the storage must be retrieved.
The storage in NVMeVirt exists linearly in memory.
To obtain the contents of the storage, it is necessary to copy from the starting address of the storage for the entire size of the storage.


\subsubsection{Processing Save and Load}

To maintain the data mentioned in Section 3.3.1 in the new SSD, it must be saved to external files and then loaded into the new SSD.
However, since NVMeVirt is a kernel module, this requires reading from and writing to files in User Space from the kernel space.
Generally, reading and writing files from kernel space to User Space is not recommended due to reasons such as kernel protection and over-reliance on User Space.
Nonetheless, in this research, the advantages gained from reading and writing files from kernel space outweigh the drawbacks, so we proceeded with this approach.


As mentioned in Section 3.3.1, the data that needs to be preserved includes the FTL metadata, block lists, SSD storage metadata, and contents.
Instead of managing these in multiple files, we opted to store them in a single file for convenience in later distribution and SSD aging creation.
The functions used for reading and writing to the file are \texttt{Kernel\_Write()} and \texttt{Kernel\_Read()}.

There are two key points to highlight.
First is the block lists.
The FTL manages blocks in list form.
Since the mapping table is created as an array, the entire array can simply be copied.
However, for linked lists, this is not possible.
To copy a linked list, we must traverse the list and save the information in order.
Care must also be taken during loading; in the new SSD, all blocks exist in a Free Block state and are included in the Free Block List.
Even if multiple read and write operations have been conducted on the new SSD, the blocks included in the Free Block List and Full Block List will differ from those in the aged SSD.
Therefore, all lists must be initialized before loading.

The second point is the storage data saving process.
While the storage may contain contents across all ranges, it may also have empty spaces.
If we were to save the entire range of the storage to the file, it could include empty spaces, making it inefficient.
Thus, we aim to skip the empty spaces and only save the areas where data is present in the file.
This can be easily implemented using the mapping table.
The mapping table is stored in an array format, with the index representing the Logical Page Number (LPN) and the stored value representing the Physical Page Number (PPA).
NVMeVirt uses the LPN to store data in the storage and can verify whether data is stored in a given page using the PPA.
Therefore, by traversing the mapping table, we check whether data is stored in the page via the PPA, and if it is, we save the page's size to the file using the LPN.


The loading process also utilizes the mapping table.
Similarly, since the data for the mapping table is saved and loaded together with the file, we can confirm the pages using the PPA and attempt to load the data into storage using the LPN.
This approach allows us to avoid storing unnecessary data, thereby reducing the file size for distribution and saving time.
